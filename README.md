# ğŸ§  Next Word Predictor using LSTM  
A deep learning project that predicts the **next word** in a sentence using an **LSTM-based Neural Network**.  
This project demonstrates text preprocessing, sequence generation, model training, and real-time prediction.

---

## ğŸš€ Features
- Trains an LSTM model on a custom text dataset  
- Uses Tokenizer for text preprocessing  
- Creates input-output sequences for next-word prediction  
- Embedding + LSTM + Dense architecture  
- Predicts the next word based on user input  
- Includes a Jupyter Notebook for clarity and experimentation  

---


## ğŸ“‚ Project Structure

Next-Word-Predictor/
â”‚
â”œâ”€â”€ Next_Word_Predictor.ipynb # Main notebook
â”œâ”€â”€ data.txt # Training dataset
â”œâ”€â”€ README.md # Documentation
â””â”€â”€ models/ # (Optional) saved models

---

## ğŸ› ï¸ Technologies Used
- Python  
- TensorFlow / Keras  
- NumPy  
- Pandas  
- Jupyter Notebook  

---

## ğŸ“¥ Installation

### 1ï¸âƒ£ Clone the repository
bash
git clone https://github.com/KunjanMinama/Next-Word-Predictor.git
cd Next-Word-Predictor
2ï¸âƒ£ Install dependencies
bash
Copy code
pip install tensorflow numpy pandas
ğŸ“Š How It Works
1. Load Dataset
Reads text from data.txt

Converts text to lowercase

Removes unwanted characters

2. Tokenization
python
Copy code
tokenizer = Tokenizer()
tokenizer.fit_on_texts(corpus)
3. Create sequences
Example:

kotlin
Copy code
deep learning is fun
becomes training sequences like:

csharp
Copy code
deep learning
deep learning is
4. Pad sequences
Ensures equal input length.

5. Build LSTM Model
python
Copy code
model = Sequential()
model.add(Embedding(vocab_size, 128, input_length=max_len))
model.add(LSTM(150, return_sequences=True))
model.add(LSTM(100))
model.add(Dense(vocab_size, activation='softmax'))
6. Train the model
python
Copy code
model.fit(X, y, epochs=50, batch_size=64)
7. Predict next word
Input:

arduino
Copy code
"deep learning is"
Output:

arduino
Copy code
"powerful"
ğŸ§ª Example Prediction Code
python
Copy code
def predict_next_word(model, tokenizer, text, max_len):
    for _ in range(1):
        token_list = tokenizer.texts_to_sequences([text])[0]
        token_list = pad_sequences([token_list], maxlen=max_len-1, padding='pre')
        predicted = model.predict(token_list, verbose=0)
        next_index = np.argmax(predicted)
        for word, index in tokenizer.word_index.items():
            if index == next_index:
                return word
ğŸ“ˆ Results
LSTM learns sentence patterns well

Predicts meaningful next words

Works better with larger datasets

ğŸ’¡ Future Improvements
Add Bidirectional LSTM

Replace LSTM with Transformer

Add GUI using Streamlit/Gradio

Train on large text corpora

ğŸ¤ Contributing
Pull requests are welcome.
If you find any issues, feel free to open an issue.

â­ Support
If you like this project, please give it a star â­ on GitHub.
It motivates further development!

ğŸ§‘â€ğŸ’» Author
Kunjan Minama
AI/ML Developer | Deep Learning | NLP

yaml
Copy code

---

# Want a **professional GitHub banner**, project logo, or badges (build/accuracy/stars)?  
I can generate and design them for you.






